{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "41ce62a8-251f-4f9e-b375-e93a5861c3fe",
      "metadata": {
        "id": "41ce62a8-251f-4f9e-b375-e93a5861c3fe"
      },
      "source": [
        "# Rag From Scratch: Overview\n",
        "\n",
        "These notebooks walk through the process of building logal RAG app(s) from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0aee0dc8",
      "metadata": {
        "id": "0aee0dc8"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "# Suppress all warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b140fe7",
      "metadata": {
        "id": "9b140fe7"
      },
      "source": [
        "### Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3a88555a-53a5-4ab8-ba3d-e6dd3a26c71a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a88555a-53a5-4ab8-ba3d-e6dd3a26c71a",
        "outputId": "9419d9cc-b23a-4cb2-b54b-3f7d55a02080"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: ollama in /usr/local/lib/python3.10/dist-packages (0.3.3)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.5)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.127)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.5.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from ollama) (0.27.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.11.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain_community) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain_community) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (1.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# Installing Required Packages\n",
        "! pip install langchain_community tiktoken langchain ollama faiss-cpu sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Ollama on linux"
      ],
      "metadata": {
        "id": "scU7v-jrGy_d"
      },
      "id": "scU7v-jrGy_d"
    },
    {
      "source": [
        "! curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4ndV8-QGR-f",
        "outputId": "05c86368-e90d-46a0-e8f1-394c2d4a0fa7"
      },
      "id": "L4ndV8-QGR-f",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Ollama Server in the Background**\n",
        "\n",
        "The nohup **(no hangup**) command will prevent the process from stopping when the notebook cell finishes or when the Colab runtime disconnects. You should also redirect both the output and error to a log file."
      ],
      "metadata": {
        "id": "JYlFE4JiIhqm"
      },
      "id": "JYlFE4JiIhqm"
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve > ollama.log 2>&1 &"
      ],
      "metadata": {
        "id": "0rF4llkSLuEs"
      },
      "id": "0rF4llkSLuEs",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1eae0ab7-d43b-43e0-8b99-6122a636fe0c",
      "metadata": {
        "id": "1eae0ab7-d43b-43e0-8b99-6122a636fe0c"
      },
      "source": [
        "## Part 1: Running Local Models with Ollama and FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "34f6b986",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34f6b986",
        "outputId": "77381a96-16dd-4652-849e-6925136e937e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 8eeb52dfb3bb... 100% ▕▏ 4.7 GB                         \n",
            "pulling 948af2743fc7... 100% ▕▏ 1.5 KB                         \n",
            "pulling 0ba8f0e314b4... 100% ▕▏  12 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 1a4c3c319823... 100% ▕▏  485 B                         \n",
            "verifying sha256 digest \n",
            "writing manifest \n",
            "success \u001b[?25h\n"
          ]
        }
      ],
      "source": [
        "# Pull LLaMA 3.1 model using Ollama (local language model)\n",
        "! ollama pull llama3.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bff6b99e",
      "metadata": {
        "id": "bff6b99e"
      },
      "source": [
        "#### 1. Import necessary libraries\n",
        "\n",
        "This section imports key libraries necessary for building a **local Retrieval-Augmented Generation (RAG) pipeline**:\n",
        "\n",
        "1. **Ollama**: Used to load and interact with local instances of the **LLaMA** model, allowing for local language model inference.\n",
        "2. **FAISS**: A highly efficient vector store for performing fast similarity searches and document retrieval in-memory.\n",
        "3. **HuggingFaceEmbeddings**: Provides local text embeddings using models from Hugging Face, enabling semantic search and document vectorization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d635cb54",
      "metadata": {
        "id": "d635cb54"
      },
      "outputs": [],
      "source": [
        "import ollama\n",
        "from langchain.llms import Ollama\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d806c8d5",
      "metadata": {
        "id": "d806c8d5"
      },
      "source": [
        "#### 2. Load the local LLM\n",
        "\n",
        "This line initializes the **LLaMA 3.1** model using **Ollama**:\n",
        "\n",
        "- **llm = Ollama(model=\"llama3.1\")**: Loads the **LLaMA 3.1** language model locally through the **Ollama** library. This model allows for performing natural language generation and other tasks entirely offline, leveraging the power of large language models without relying on external services or APIs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8c5e65c2",
      "metadata": {
        "id": "8c5e65c2"
      },
      "outputs": [],
      "source": [
        "llm = Ollama(model=\"llama3.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6be34e58",
      "metadata": {
        "id": "6be34e58"
      },
      "source": [
        "#### 3. Use local embeddings for retrieval\n",
        "\n",
        "This line initializes the **Hugging Face embeddings** model:\n",
        "\n",
        "- **embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")**: Loads the `all-mpnet-base-v2` model from Hugging Face's **sentence-transformers** library to generate text embeddings locally. These embeddings represent the semantic meaning of text and are essential for tasks like similarity search and document retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0f95b61b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f95b61b",
        "outputId": "b81fab14-ce29-4d49-d379-b5fb7cf780be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-3a8dd36e4865>:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n"
          ]
        }
      ],
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e133987",
      "metadata": {
        "id": "6e133987"
      },
      "source": [
        "#### 4. Load your documents\n",
        "\n",
        "This line defines a list of sample text documents for embedding and retrieval:\n",
        "\n",
        "- **texts = [\"Sample text 1\", \"Sample text 2\", \"Sample text 3\"]**: This creates a list of text strings that will be embedded and stored in the vector store. These sample texts are used to simulate documents or pieces of information that the system will later search through based on similarity to a query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "22caac52",
      "metadata": {
        "id": "22caac52"
      },
      "outputs": [],
      "source": [
        "texts = [\"Sample text 1\", \"Sample text 2\", \"Sample text 3\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4529cb8b",
      "metadata": {
        "id": "4529cb8b"
      },
      "source": [
        "#### 5. Create the FAISS vectorstore locally\n",
        "\n",
        "This line creates a **FAISS vector store** from the provided texts:\n",
        "\n",
        "- **faiss_index = FAISS.from_texts(texts, embeddings)**: Converts the `texts` into vector embeddings using the specified embedding model (`embeddings`) and stores them in a **FAISS** vector index. FAISS is used for efficient nearest-neighbor search and similarity retrieval based on the vector representations of the texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a3565412",
      "metadata": {
        "id": "a3565412"
      },
      "outputs": [],
      "source": [
        "faiss_index = FAISS.from_texts(texts, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "045b9324",
      "metadata": {
        "id": "045b9324"
      },
      "source": [
        "#### 6. Create the retrieval-based chain\n",
        "\n",
        "This function defines the **local Retrieval-Augmented Generation (RAG) pipeline**:\n",
        "\n",
        "- **`def local_rag(question, faiss_index, llm):`**: This function takes in a `question`, the FAISS vector store (`faiss_index`), and the local LLaMA model (`llm`) to generate a response based on the most relevant documents.\n",
        "\n",
        "1. **Document Retrieval**:\n",
        "    - **`docs = faiss_index.similarity_search(question)`**: The function performs a similarity search in the FAISS index using the provided question. It retrieves documents that are semantically closest to the query.\n",
        "   \n",
        "2. **Prompt Construction**:\n",
        "    - **`prompt = f\"Context: {docs}\\n\\nQuestion: {question}\\n\\nAnswer:\"`**: Constructs a prompt that includes the retrieved documents as context, along with the original question. This prompt is then passed to the LLaMA model for generation.\n",
        "\n",
        "3. **Response Generation**:\n",
        "    - **`response = llm(prompt, clean_up_tokenization_spaces=False)`**: The local LLaMA model generates an answer based on the provided context and question. The `clean_up_tokenization_spaces=False` parameter is set to prevent tokenization-related warnings from the transformers library.\n",
        "\n",
        "4. **Return Response**:\n",
        "    - **`return response`**: The generated answer is returned as the output of the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a4d19069",
      "metadata": {
        "id": "a4d19069"
      },
      "outputs": [],
      "source": [
        "def local_rag(question, faiss_index, llm):\n",
        "    # Retrieve relevant documents\n",
        "    docs = faiss_index.similarity_search(question)\n",
        "\n",
        "    # Create the prompt with retrieved documents\n",
        "    prompt = f\"Context: {docs}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "\n",
        "    # Set clean_up_tokenization_spaces to False in transformers to avoid future warnings\n",
        "    response = llm(prompt, clean_up_tokenization_spaces=False)\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1087779a",
      "metadata": {
        "id": "1087779a"
      },
      "source": [
        "#### Run the RAG Pipeline:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07d9dd83",
      "metadata": {
        "id": "07d9dd83"
      },
      "source": [
        "**1. `question = \"What is Retrieval-Augmented Generation (RAG)?\"`**:\n",
        "Defines the input question to be answered by the system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "42c4b3d7",
      "metadata": {
        "id": "42c4b3d7"
      },
      "outputs": [],
      "source": [
        "question = \"What is Retrieval-Augmented Generation (RAG)?\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "345ded6b",
      "metadata": {
        "id": "345ded6b"
      },
      "source": [
        "**2. `response = local_rag(question, faiss_index, llm)`**: The function retrieves relevant documents based on the question, constructs a prompt, and generates a response from the local LLaMA model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "74b6e230",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74b6e230",
        "outputId": "bc079184-6ada-4079-c7c4-c0d38cd9d833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-7ca37b86d91d>:9: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = llm(prompt, clean_up_tokenization_spaces=False)\n"
          ]
        }
      ],
      "source": [
        "response = local_rag(question, faiss_index, llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf440a91",
      "metadata": {
        "id": "bf440a91"
      },
      "source": [
        "Output the response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f0374b48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0374b48",
        "outputId": "adf14ca5-7d10-4e68-edca-88dbf592646b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval-Augmented Generation (RAG) is a type of model that combines the strengths of two techniques:\n",
            "\n",
            "1. **Retrieval**: This involves searching through a large database or corpus to find relevant information related to a given query or prompt.\n",
            "2. **Generation**: This involves using a language model to generate new text based on the input prompt or query.\n",
            "\n",
            "In RAG, the retrieval step is used to gather a set of relevant documents or passages from a large corpus, and then these retrieved passages are used as input to a generation model. The generation model then uses this input to produce a coherent and informative response to the original query or prompt.\n",
            "\n",
            "RAG models have been particularly useful in applications such as:\n",
            "\n",
            "* Text summarization: RAG can be used to retrieve relevant passages from a large corpus and then summarize them into a concise and accurate summary.\n",
            "* Conversational AI: RAG can be used to generate responses to user queries by retrieving relevant information from a large database and then generating a response based on that information.\n",
            "\n",
            "In the context of the provided code snippet, RAG could potentially be used to retrieve relevant documents (e.g., articles or research papers) related to specific topics or keywords, and then use these retrieved documents as input to generate new text, such as summaries or abstracts.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Efficient Document Indexing with FAISS\n",
        "\n",
        "- **Hugging Face Embeddings**: Loads the `sentence-transformers/all-mpnet-base-v2` model to embed both a `question` and a `document`. This allows for vectorizing text for similarity search.\n",
        "    ```python\n",
        "    embd = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "    query_result = embd.embed_query(question)\n",
        "    document_result = embd.embed_query(document)\n",
        "    ```\n",
        "\n",
        "- **Cosine Similarity**: Measures how similar the embedded vectors of the `question` and `document` are. A value of `1` indicates identical vectors.\n",
        "    ```python\n",
        "    similarity = cosine_similarity(query_result, document_result)\n",
        "    ```\n"
      ],
      "metadata": {
        "id": "2bOL_BwrLJsU"
      },
      "id": "2bOL_BwrLJsU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Hugging Face Embeddings for local use\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Example documents\n",
        "question = \"What kinds of pets do I like?\"\n",
        "document = \"My favorite pet is a cat.\"\n",
        "\n",
        "# Embed the query and document using Hugging Face embeddings\n",
        "embd = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "query_result = embd.embed_query(question)\n",
        "document_result = embd.embed_query(document)\n",
        "\n",
        "# Calculate the length of the query embeddings\n",
        "len(query_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55sGNZmPMmHI",
        "outputId": "9a126c08-822b-438e-c0d0-64b5bbc153d7"
      },
      "id": "55sGNZmPMmHI",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement cosine similarity for comparing query and document embeddings\n",
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "similarity = cosine_similarity(query_result, document_result)\n",
        "print(\"Cosine Similarity:\", similarity)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2GF_gKzMtaG",
        "outputId": "7a9560f8-2bdd-485f-a23d-59bafe5c5889"
      },
      "id": "D2GF_gKzMtaG",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 0.5595268748748163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3: Document Splitting and Vectorization\n",
        "\n",
        "- **Document Loader and Splitting**: Loads blog content from a URL and splits it into smaller chunks using the `RecursiveCharacterTextSplitter` to prepare for vectorization. This ensures efficient retrieval of smaller document chunks based on the query.\n",
        "    ```python\n",
        "    loader = WebBaseLoader(...)\n",
        "    splits = text_splitter.split_documents(blog_docs)\n",
        "    ```\n",
        "\n",
        "- **FAISS Vector Store**: Embeds the split documents using Hugging Face embeddings and stores them in a FAISS vector store. This vector store enables fast retrieval of the most relevant documents based on similarity to the query.\n",
        "    ```python\n",
        "    vectorstore = FAISS.from_texts(texts=texts, embedding=embedding_model)\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
        "    ```"
      ],
      "metadata": {
        "id": "2eEVXV2lLNrD"
      },
      "id": "2eEVXV2lLNrD"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load blog posts and use RecursiveCharacterTextSplitter for splitting documents\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "import bs4\n",
        "\n",
        "# Load blog from web\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))\n",
        "    )\n",
        ")\n",
        "blog_docs = loader.load()"
      ],
      "metadata": {
        "id": "9Dd0PrndNDV2"
      },
      "id": "9Dd0PrndNDV2",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the blog documents\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "splits = text_splitter.split_documents(blog_docs)"
      ],
      "metadata": {
        "id": "D4FOL30JNRup"
      },
      "id": "D4FOL30JNRup",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use FAISS for local vector storage and retrieval\n",
        "texts = [doc.page_content for doc in splits]\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
      ],
      "metadata": {
        "id": "XGgLTEZbNUsB"
      },
      "id": "XGgLTEZbNUsB",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create FAISS vectorstore\n",
        "vectorstore = FAISS.from_texts(texts=texts, embedding=embedding_model)"
      ],
      "metadata": {
        "id": "oBoxomlmNVoK"
      },
      "id": "oBoxomlmNVoK",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the vectorstore into a retriever with search options\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
      ],
      "metadata": {
        "id": "PrTUaYf5NXeC"
      },
      "id": "PrTUaYf5NXeC",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test document retrieval\n",
        "docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")\n",
        "print(\"Number of relevant documents:\", len(docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j37XNUqPNCX3",
        "outputId": "0a54dfd7-e25f-4790-a9d1-0ca9f8177ffc"
      },
      "id": "j37XNUqPNCX3",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of relevant documents: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9122525b",
      "metadata": {
        "id": "9122525b"
      },
      "source": [
        "### Part 4: Generating Responses with Local LLaMA\n",
        "\n",
        "- **LLaMA Model**: Loads the local LLaMA model using **Ollama** and defines a **prompt template** to generate answers based on the retrieved documents.\n",
        "    ```python\n",
        "    llm = Ollama(model=\"llama3.1\")\n",
        "    prompt = ChatPromptTemplate.from_template(template)\n",
        "    ```\n",
        "\n",
        "- **RAG Chain**: Combines document retrieval and generation by passing the retrieved documents as context to the LLaMA model to generate a detailed response.\n",
        "    ```python\n",
        "    rag_chain = (\n",
        "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    response = rag_chain.invoke(\"What is Task Decomposition?\")\n",
        "    ```\n",
        "    \n",
        "This flow demonstrates how the system embeds documents, retrieves relevant information, and generates responses using a fully local pipeline with FAISS, Hugging Face embeddings, and the LLaMA model.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import Ollama\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Initialize the local LLaMA model using Ollama\n",
        "llm = Ollama(model=\"llama3.1\")\n",
        "\n",
        "# Define the prompt template for generating responses\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Example usage with retrieved context and a question\n",
        "context = \"This is the context text for the question.\"\n",
        "question = \"What is the main topic of the context?\"\n",
        "\n",
        "response = llm(prompt.format(context=context, question=question))\n",
        "print(\"Generated Response:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmmG2JXUNuXw",
        "outputId": "28580521-4bfc-477f-d443-a235269577cf"
      },
      "id": "YmmG2JXUNuXw",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Response: There is no context provided. The statement says \"This is the context text for the question\", but there is no actual text or information to draw from. If you provide a context, I'll be happy to help!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a more advanced RAG chain using retrieved documents and the local LLaMA model\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "response = rag_chain.invoke(\"What is Task Decomposition?\")\n",
        "print(\"RAG Chain Response:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv_l4ieqNqas",
        "outputId": "66614c55-96fd-48ad-89b2-2f37a22edce7"
      },
      "id": "zv_l4ieqNqas",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG Chain Response: Task decomposition refers to the process of breaking down a complex user request into multiple manageable tasks, each with its own attributes such as type, ID, dependencies, and arguments. This process is done by a Large Language Model (LLM) in the first stage of the system described in the context, known as Task planning.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}